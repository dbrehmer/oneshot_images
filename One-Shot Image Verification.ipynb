{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI8AAACQCAYAAAAx4zjdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1243b0350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "face_number = 79\n",
    "plt.figure(figsize=(2.0, 2.26))\n",
    "plt.imshow(data.images[face_number],\n",
    "           cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, MaxPooling2D, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shared layers. This portion of the model will compute latent variables\n",
    "# for each of the images to be compared in the next stage.\n",
    "encode_image = Sequential()\n",
    "encode_image.add(Conv2D(64, (9, 9), activation='relu', bias_initializer=RandomNormal(0.5, .01), input_shape=(64, 64)))\n",
    "encode_image.add(MaxPooling2D((2, 2))) # 28x28\n",
    "encode_image.add(Conv2D(128, (5, 5), activation='relu', bias_initializer=RandomNormal(0.0, .01)))\n",
    "encode_image.add(MaxPooling2D((2, 2))) # 12x12\n",
    "encode_image.add(Conv2D(128, (3, 3), activation='relu', bias_initializer=RandomNormal(0.0, .01)))\n",
    "encode_image.add(MaxPooling2D((2, 2)))\n",
    "encode_image.add(Conv2D(256, (4, 4), activation='relu', bias_initializer=RandomNormal(0.0, .01)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Olivetti face database is not large enough to do well training the network and the architecture of this netwrk is really optimized for character recognition, it probably makes more sense to use a pretrained face recognition network for encoding the images. The result of this encoding will then be compared between the two and the final stage will be trained to complete the discrimination.\n",
    "\n",
    "The best choice for pretrained face recognition networks is probably the VGG-Face network, as described in this paper - http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf, which has already been converted to Keras here - https://gist.github.com/EncodeTS/6bbe8cb8bebad7a672f0d872561782d9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difficulty may be the differning format between the VGG face dataset and the Olivetti set. To accomodate the different format and grayscale the Olivetti images will be adjusted to match the expected input with a similar placement of the face in the image and the grayscale repeated on all three rbg planes.\n",
    "\n",
    "The VGG-Faces based siamese model will be fine tuned to the first 30 of the Olivetti faces and the remaining 10 will be used to test one-shot accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I was trying to determine the way faces should be framed in the image I found this paper which describes adapting the VGG-Faces model for cross-over learning - http://cs231n.stanford.edu/reports/2016/pdfs/006_Report.pdf\n",
    "\n",
    "It seems that the Olivetti faces are probably reasonably well cropped, so we will only need to change them to RGB format for the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vgg_face(weights_path=None):\n",
    "    img = Input(shape=(224, 224, 3))\n",
    "\n",
    "    conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_1', trainable=False)(img)\n",
    "    conv1_2 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_2', trainable=False)(conv1_1)\n",
    "    pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool1', trainable=False)(conv1_2)\n",
    "\n",
    "    conv2_1 = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_1', trainable=False)(pool1)\n",
    "    conv2_2 = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_2', trainable=False)(conv2_1)\n",
    "    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2', trainable=False)(conv2_2)\n",
    "\n",
    "    conv3_1 = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_1', trainable=False)(pool2)\n",
    "    conv3_2 = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_2', trainable=False)(conv3_1)\n",
    "    conv3_3 = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_3', trainable=False)(conv3_2)\n",
    "    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3', trainable=False)(conv3_3)\n",
    "\n",
    "    conv4_1 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_1', trainable=False)(pool3)\n",
    "    conv4_2 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_2', trainable=False)(conv4_1)\n",
    "    conv4_3 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_3', trainable=False)(conv4_2)\n",
    "    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4', trainable=False)(conv4_3)\n",
    "\n",
    "    conv5_1 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_1', trainable=False)(pool4)\n",
    "    conv5_2 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_2', trainable=False)(conv5_1)\n",
    "    conv5_3 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_3', trainable=False)(conv5_2)\n",
    "    pool5 = MaxPooling2D((2, 2), strides=(2, 2), name='pool5', trainable=False)(conv5_3)\n",
    "\n",
    "    flat = Flatten()(pool5)\n",
    "    fc6 = Dense(4096, activation='relu', name='fc6', trainable=False)(flat)\n",
    "    fc7 = Dense(4096, activation='relu', name='fc7', trainable=False)(fc6)\n",
    "    out = Dense(2622, activation='softmax', name='fc8')(fc7)\n",
    "\n",
    "    model = Model(inputs=img, outputs=out)\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# encode_image = vgg_face('rcmalli_vggface_tf_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc7 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 2622)              10742334  \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 10,742,334\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encode_image.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_image(img):\n",
    "    new_image = Image.fromarray(img).resize((224, 224))\n",
    "    img = np.asarray(new_image)\n",
    "    color_image = np.array([img, img, img])\n",
    "    return transpose(color_image, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJUAAACQCAYAAAABbyn8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZkg1g2OH/w/wCk4dTSmR1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1252d13d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2.0, 2.26))\n",
    "plt.imshow(convert_image(data.images[79]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was looking for how to merge the results form the twins and found this blog post on how someone implemented the model in the paper - https://sorenbouma.github.io/blog/oneshot/ So, some of the following code is adapted from that. Unfortunately, I found his code fairly opaque, due to minimal comments or whitespace, but I did get some clues from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_branch_model():\n",
    "    # Set up VGG-Face network and load weights.\n",
    "    encode_image = vgg_face('rcmalli_vggface_tf_v2.h5')\n",
    "\n",
    "    # Strip off the final layer used for prediction, leaving the image feature vector.\n",
    "    # Unfortunately, popping off layers still does not work correctly. After popping\n",
    "    # you still need to reassign the output of the model to the final layer or it will\n",
    "    # still be trying to look at the old final layer. See here - https://github.com/fchollet/keras/issues/2371\n",
    "    # encode_image.layers.pop()\n",
    "    encode_image.layers.pop()\n",
    "    encode_image.outputs = [encode_image.layers[-1].output]\n",
    "    encode_image.layers[-1].outbound_nodes = []\n",
    "\n",
    "    return encode_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encode = get_branch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc7 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 0\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encode.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(0.00006)\n",
    "encode.compile(loss=\"binary_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = np.zeros((400, 224, 224, 3))\n",
    "for i in range(400):\n",
    "    images[i, :, :, :] = convert_image(data.images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors = encode.predict(images[:16], batch_size = 16)\n",
    "for i in range(24):\n",
    "    vectors = np.concatenate((vectors, encode.predict(images[16*(i+1):16*(i+2)], batch_size = 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 4096)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.02642416  0.01712929  0.          0.          0.        ]\n",
      " [ 0.          0.          0.0100847   0.          0.          0.        ]\n",
      " [ 0.          0.0043798   0.          0.          0.          0.        ]\n",
      " [ 0.          0.00726469  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.01590716  0.          0.          0.          0.        ]\n",
      " [ 0.          0.03807902  0.          0.          0.          0.        ]\n",
      " [ 0.          0.01726065  0.          0.          0.          0.        ]\n",
      " [ 0.          0.05755867  0.          0.          0.          0.        ]\n",
      " [ 0.          0.0638838   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.00328201  0.          0.          0.        ]\n",
      " [ 0.          0.00861659  0.          0.          0.          0.        ]\n",
      " [ 0.          0.0076882   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[20:40, 30:36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we use 75% of the Olivetti Faces data set for training, the number of positive and negative pairs we can get is 9X5X30 = 1350 and 150x290 = 43,500.\n",
    "We will draw evenly and randomly from those sets for training batches and run until we are not seeing much additional improvement.\n",
    "For validation we will have 450 positive cases and 4500 negative cases. To do this more rigorously we would optimize the model to perform well on half of these data, using 5 of the remaining individuals, and validate that on the final 5. This gives us 10!/(5!x5!) = 252 ways to do the test and validate groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly choose 75% of the indices for the Olivetti subjects for the training set.\n",
    "train = np.random.choice(range(40), 30, replace=False)\n",
    "# Take the remaining 25% for the test set.\n",
    "test = np.array([idx for idx in range(40) if idx not in train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "# Pairs of indices for images of the same subject.\n",
    "test_pairs_pos = np.concatenate([np.array(list(combinations(range(10), 2)))+idx*10 for idx in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pairs_pos = np.concatenate([np.array(list(combinations(range(10), 2)))+idx*10 for idx in train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_neg_pair(subject_indices):\n",
    "    \"\"\"Takes an array of subject indices and returns a pair of indices to non-matching image vectors.\"\"\"\n",
    "    subject = random.choice(subject_indices)\n",
    "    ind1 = 10*subject + random.choice(range(10))\n",
    "    others = subject_indices.tolist()\n",
    "    others.remove(subject)\n",
    "    ind2 = 10*random.choice(others) + random.choice(range(10))\n",
    "    return [ind1, ind2]\n",
    "\n",
    "def draw_random_neg_pairs(subject_indices, number):\n",
    "    \"\"\"Returns a set of number unique negative pairs\"\"\"\n",
    "    pairs = []\n",
    "    while (len(pairs) < number):\n",
    "        pair = set(random_neg_pair(subject_indices))\n",
    "        if pair not in pairs:\n",
    "            pairs.append(pair)\n",
    "    return np.array([list(s) for s in pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pairs_neg = draw_random_neg_pairs(train, 1350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  69],\n",
       "       [ 10,  35],\n",
       "       [218, 190],\n",
       "       [218,  87],\n",
       "       [329, 203]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs_neg[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.models import Model\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "# Set up VGG-Face network and load weights.\n",
    "encode_image = vgg_face('rcmalli_vggface_tf_v2.h5')\n",
    "\n",
    "# Strip off the final two layers used for prediction, leaving the image feature vector.\n",
    "# Unfortunately, popping off layers still does not work correctly. After popping\n",
    "# you still need to reassign the output of the model to the final layer or it\n",
    "# still be trying to look at the old final layer. See here - https://github.com/fchollet/keras/issues/2371\n",
    "encode_image.layers.pop()\n",
    "encode_image.outputs = [encode_image.layers[-1].output]\n",
    "encode_image.layers[-1].outbound_nodes = []\n",
    "\n",
    "# Encode each of the two inputs into a vector with the convnet.\n",
    "branch_l = encode_image(left_input)\n",
    "branch_r = encode_image(right_input)\n",
    "\n",
    "# Merge two encoded inputs with the L1 distance between them.\n",
    "both = Lambda(lambda x: np.abs(x[0]-x[1]), output_shape=lambda x: x[0].shape)([branch_l, branch_r])\n",
    "# Make the final prediction layer as described in the paper. This is the only layer we will train to begin with.\n",
    "prediction = Dense(1, activation='sigmoid', bias_initializer=TruncatedNormal(mean=0.5, stddev=0.01))(both)\n",
    "siamese_net = Model([left_input, right_input], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discrim_input = Input((2, 4096))\n",
    "# Merge two encoded inputs with the L1 distance between them.\n",
    "both = Lambda(lambda x: np.abs(x[0]-x[1]), output_shape=lambda x: x[0].shape)(discrim_input)\n",
    "# Make the final prediction layer as described in the paper. This is the only layer we will train to begin with.\n",
    "# We can unfreeze additional fc layers and train them if it seems to be needed.\n",
    "prediction = Dense(1, activation='sigmoid', bias_initializer=TruncatedNormal(mean=0.5, stddev=0.01))(both)\n",
    "discriminator = Model(inputs=discrim_input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134264641"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(0.00006)\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paper working\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "siamese_net.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_10 (InputLayer)            (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_11 (InputLayer)            (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  multiple              134260544   input_10[0][0]                   \n",
      "                                                                   input_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 4096)          0           model_1[4][0]                    \n",
      "                                                                   model_1[5][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             4097        lambda_4[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 134,264,641\n",
      "Trainable params: 4,097\n",
      "Non-trainable params: 134,260,544\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = 'https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_v2.h5'\n",
    "\n",
    "class Siamese_Face_Tester(object):\n",
    "    \"\"\"Loads Olivetti Face data and generates sets for training and testing one-shot learning on Siamese Network.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.train, self.test = self.new_train_test_split()\n",
    "        # Pairs of indices for images of the same subject.\n",
    "        self.test_pairs_pos, self.train_pairs_pos = get_pos_pairs()\n",
    "        # Train and test sets for training the discriminator.\n",
    "        self.train_pairs = []\n",
    "        self.train_target = []\n",
    "        self.test_pairs = []\n",
    "        self.test_target = []\n",
    "        self.refresh_train_sets()\n",
    "        # Fetch and convert Olivetti Face data.\n",
    "        self.data = fetch_olivetti_faces()\n",
    "        self.images = np.zeros((400, 224, 224, 3))\n",
    "        for i in range(400):\n",
    "            self.images[i, :, :, :] = self.convert_image(self.data.images[i])\n",
    "        self.image_vectors = None\n",
    "        self.branch_encoder = None\n",
    "        self.weights_path = None\n",
    "        self.discriminator = self.get_discriminator()\n",
    "\n",
    "\n",
    "    def convert_image(self, img):\n",
    "        \"\"\"Convert gray scale image to color in [h, w, channels] format.\"\"\"\n",
    "        new_image = Image.fromarray(img).resize((224, 224))\n",
    "        img = np.asarray(new_image)\n",
    "        color_image = np.array([img, img, img])\n",
    "        return transpose(color_image, (1, 2, 0))\n",
    "\n",
    "\n",
    "    def new_train_test_split(self):\n",
    "        \"\"\"Olivetti Faces includes 40 sets of 10 images. This returns a 75/25 random split of 40 indices.\"\"\"\n",
    "        # Randomly choose 75% of the indices for the Olivetti subjects for the training set.\n",
    "        train = np.random.choice(range(40), 30, replace=False)\n",
    "        # Take the remaining 25% for the test set.\n",
    "        test = np.array([idx for idx in range(40) if idx not in train])\n",
    "        return train, test\n",
    "\n",
    "\n",
    "    def get_pos_pairs(self):\n",
    "        \"\"\"Returns all possible positive pairs for the current train-test split.\"\"\"\n",
    "        test_pairs_pos = np.concatenate([np.array(list(combinations(range(10), 2)))+idx*10 for idx in self.test])\n",
    "        train_pairs_pos = np.concatenate([np.array(list(combinations(range(10), 2)))+idx*10 for idx in self.train])\n",
    "        return test_pairs_pos, train_pairs_pos\n",
    "\n",
    "\n",
    "    def refresh_train_sets(self, batch_size=32):\n",
    "        \"\"\"Refreshes the train and test sets of index pairs and targets with a new random draw of negative pairs.\"\"\"\n",
    "        # Make the train set an integer multiple of the batch size (not sure if this matters...)\n",
    "        drop = len(self.train_pairs_pos)%batch_size\n",
    "        train_pairs = np.concatenate((self.train_pairs_pos[:-drop], draw_random_neg_pairs(self.train, len(self.train_pairs_pos)-drop)))\n",
    "        train_targets = np.concatenate((np.ones(len(train_pairs)/2), np.zeros(len(train_pairs)/2)))\n",
    "        train_labeled = np.array([train_pairs, train_targets]).T\n",
    "        np.random.shuffle(train_labeled)\n",
    "        [self.train_pairs, self.train_target] = train_labeled.T\n",
    "        # Repeat same procedure for the test set. Clearly could be refactored in some way, but no time now.\n",
    "        drop = len(self.test_pairs_pos)%batch_size\n",
    "        test_pairs = np.concatenate((self.test_pairs_pos[:-drop], draw_random_neg_pairs(self.test, len(self.test_pairs_pos)-drop)))\n",
    "        test_targets = np.concatenate((np.ones(len(test_pairs)/2), np.zeros(len(test_pairs)/2)))\n",
    "        test_labeled = np.array([test_pairs, test_targets]).T\n",
    "        np.random.shuffle(test_labeled)\n",
    "        [self.test_pairs, self.test_target] = test_labeled.T\n",
    "        \n",
    "        \"\"\"The test set accuracy during training is equivalent to a one-out-of-two one-shot learning task averaged over the all test data.\"\"\"\n",
    "        \n",
    "\n",
    "    def random_neg_pair(self, subject_indices):\n",
    "        \"\"\"Takes an array of subject indices and returns a pair of indices to non-matching image vectors.\"\"\"\n",
    "        subject = random.choice(subject_indices)\n",
    "        ind1 = 10*subject + random.choice(range(10))\n",
    "        others = subject_indices.tolist()\n",
    "        others.remove(subject)\n",
    "        ind2 = 10*random.choice(others) + random.choice(range(10))\n",
    "        return [ind1, ind2]\n",
    "\n",
    "\n",
    "    def draw_random_neg_pairs(self, subject_indices, number):\n",
    "        \"\"\"Returns a set of number unique negative pairs\"\"\"\n",
    "        pairs = []\n",
    "        while (len(pairs) < number):\n",
    "            pair = set(random_neg_pair(subject_indices))\n",
    "            if pair not in pairs:\n",
    "                pairs.append(pair)\n",
    "        return np.array([list(s) for s in pairs])\n",
    "\n",
    "\n",
    "    def vgg_face(self, weights_path=None):\n",
    "        img = Input(shape=(224, 224, 3))\n",
    "\n",
    "        conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_1', trainable=False)(img)\n",
    "        conv1_2 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_2', trainable=False)(conv1_1)\n",
    "        pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool1', trainable=False)(conv1_2)\n",
    "\n",
    "        conv2_1 = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_1', trainable=False)(pool1)\n",
    "        conv2_2 = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_2', trainable=False)(conv2_1)\n",
    "        pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2', trainable=False)(conv2_2)\n",
    "\n",
    "        conv3_1 = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_1', trainable=False)(pool2)\n",
    "        conv3_2 = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_2', trainable=False)(conv3_1)\n",
    "        conv3_3 = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_3', trainable=False)(conv3_2)\n",
    "        pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3', trainable=False)(conv3_3)\n",
    "\n",
    "        conv4_1 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_1', trainable=False)(pool3)\n",
    "        conv4_2 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_2', trainable=False)(conv4_1)\n",
    "        conv4_3 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_3', trainable=False)(conv4_2)\n",
    "        pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4', trainable=False)(conv4_3)\n",
    "\n",
    "        conv5_1 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_1', trainable=False)(pool4)\n",
    "        conv5_2 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_2', trainable=False)(conv5_1)\n",
    "        conv5_3 = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_3', trainable=False)(conv5_2)\n",
    "        pool5 = MaxPooling2D((2, 2), strides=(2, 2), name='pool5', trainable=False)(conv5_3)\n",
    "\n",
    "        flat = Flatten()(pool5)\n",
    "        fc6 = Dense(4096, activation='relu', name='fc6', trainable=False)(flat)\n",
    "        fc7 = Dense(4096, activation='relu', name='fc7', trainable=False)(fc6)\n",
    "        out = Dense(2622, activation='softmax', name='fc8')(fc7)\n",
    "\n",
    "        model = Model(inputs=img, outputs=out)\n",
    "\n",
    "        if weights_path:\n",
    "            model.load_weights(weights_path)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_branch_model(self):\n",
    "        \"\"\"Returns a model that can be used to encode images into vectors using the trained VGG-Face model.\"\"\"\n",
    "        if self.branch_encoder == None:\n",
    "            # Set up VGGFace network and load weights.\n",
    "            if self.weights_path == None:\n",
    "                self.get_VGG_weights()\n",
    "            encode_image = vgg_face(self.weights_path)\n",
    "\n",
    "            # Strip off the final layer used for prediction, leaving the image feature vector.\n",
    "            # Unfortunately, popping off layers still does not work correctly. After popping\n",
    "            # you still need to reassign the output of the model to the final layer or it will\n",
    "            # still be trying to look at the old final layer. See here - https://github.com/fchollet/keras/issues/2371\n",
    "            encode_image.layers.pop()\n",
    "            encode_image.outputs = [encode_image.layers[-1].output]\n",
    "            encode_image.layers[-1].outbound_nodes = []\n",
    "            self.branch_encoder = encode_image\n",
    "        return self.branch_encoder\n",
    "    \n",
    "    \n",
    "    def encode_image_vectors(self):\n",
    "        \"\"\"Uses the image encoding branch to turn the Olivetti images into vectors.\"\"\"\n",
    "        encoder = self.get_branch_model()\n",
    "        vectors = encoder.predict(self.images[:16], batch_size = 16)\n",
    "        for i in range(24):\n",
    "            vectors = np.concatenate((vectors, encoder.predict(self.images[16*(i+1):16*(i+2)], batch_size = 16)))\n",
    "        return vectors\n",
    "\n",
    "\n",
    "    def get_image_vectors(self):\n",
    "        \"\"\"Returns the image vectors, computing them if necessary.\"\"\"\n",
    "        if self.image_vectors == None:\n",
    "            self.image_vectors = self.encode_image_vectors()\n",
    "        return self.image_vectors\n",
    "    \n",
    "    \n",
    "    def get_VGG_weights(self):\n",
    "        \"\"\"Download VGGFace weights if they are not present.\"\"\"\n",
    "        self.weights_path = get_file('rcmalli_vggface_tf_v2.h5',\n",
    "                                     WEIGHTS_PATH,\n",
    "                                     cache_subdir='models')\n",
    "        \n",
    "        \n",
    "    def get_discriminator(self):\n",
    "        \"\"\"The discriminator is the part of the siamese network that we will train to tell the difference between encoded image vectors.\"\"\"\n",
    "        discrim_input = Input((2, 4096))\n",
    "        # Merge two encoded inputs with the L1 distance between them.\n",
    "        both = Lambda(lambda x: np.abs(x[0]-x[1]), output_shape=lambda x: x[0].shape)(discrim_input)\n",
    "        # Make the final prediction layer as described in the paper. This is the only layer we will train to begin with.\n",
    "        # We can unfreeze additional fc layers and train them if it seems to be needed.\n",
    "        prediction = Dense(1, activation='sigmoid', bias_initializer=TruncatedNormal(mean=0.5, stddev=0.01))(both)\n",
    "        discriminator = Model(inputs=discrim_input, outputs=prediction)\n",
    "        return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tester = Siamese_Face_Tester()\n",
    "vectors = tester.get_image_vectors()\n",
    "tester.discriminator.compile(loss=\"binary_crossentropy\", optimizer=Adam)\n",
    "X_train = [[vectors[pair[0]], vectors[pair[1]]] for pair in tester.train_pairs]\n",
    "X_test = [[vectors[pair[0]], vectors[pair[1]]] for pair in tester.test_pairs]\n",
    "tester.fit(x=X_train, y=tester.train_target, validation_data=(X_test, tester.test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Siamese_Loader:\n",
    "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
    "    def __init__(self, Xtrain, Xval):\n",
    "        self.Xval = Xval\n",
    "        self.Xtrain = Xtrain\n",
    "        self.n_classes, self.n_examples, self.w, self.h = Xtrain.shape\n",
    "        self.n_val, self.n_ex_val, _, _ = Xval.shape\n",
    "\n",
    "    def get_batch(self,n):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        categories = rng.choice(self.n_classes, size=(n,), replace=False)\n",
    "        pairs=[np.zeros((n, self.h, self.w, 1)) for i in range(2)]\n",
    "        targets=np.zeros((n, ))\n",
    "        targets[n//2:] = 1\n",
    "        for i in range(n):\n",
    "            category = categories[i]\n",
    "            idx_1 = rng.randint(0, self.n_examples)\n",
    "            pairs[0][i, :, :, :] = self.Xtrain[category, idx_1].reshape(self.w, self.h,1)\n",
    "            idx_2 = rng.randint(0, self.n_examples)\n",
    "            #pick images of same class for 1st half, different for 2nd\n",
    "            category_2 = category if i >= n//2 else (category + rng.randint(1, self.n_classes)) % self.n_classes\n",
    "            pairs[1][i, :, :, :] = self.Xtrain[category_2, idx_2].reshape(self.w, self.h,1)\n",
    "        return pairs, targets\n",
    "\n",
    "    def make_oneshot_task(self,N):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "        categories = rng.choice(self.n_val,size=(N,),replace=False)\n",
    "        indices = rng.randint(0,self.n_ex_val,size=(N,))\n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = rng.choice(self.n_examples,replace=False,size=(2,))\n",
    "        test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,1)\n",
    "        support_set = self.Xval[categories,indices,:,:]\n",
    "        support_set[0,:,:] = self.Xval[true_category,ex2]\n",
    "        support_set = support_set.reshape(N,self.w,self.h,1)\n",
    "        pairs = [test_image,support_set]\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        return pairs, targets\n",
    "\n",
    "    def test_oneshot(self,model,N,k,verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        pass\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} unique {} way one-shot learning tasks ...\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == 0:\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
    "        return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_every = 7000\n",
    "loss_every=300\n",
    "batch_size = 32\n",
    "N_way = 20\n",
    "n_val = 550\n",
    "siamese_net.load_weights(\"PATH\")\n",
    "best = 76.0\n",
    "for i in range(900000):\n",
    "    (inputs, targets)=loader.get_batch(batch_size)\n",
    "    loss=siamese_net.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        val_acc = loader.test_oneshot(siamese_net, N_way, n_val, verbose=True)\n",
    "        if val_acc >= best:\n",
    "            print(\"saving\")\n",
    "            siamese_net.save('PATH')\n",
    "            best=val_acc\n",
    "\n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration {}, training loss: {:.2f},\".format(i,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Seems that for training the final layer it may be faster to precompute the output of the VGG-Face branches for all the images and train the final layer on its own. I will see if I can figure out how to do that once I have things set up.\n",
    "\n",
    "To multiply the amount of data we have we migth also just do some transformations on the image vectors for training the final layer rather than on the original images. Perhaps a dropout of 10% of the components. If we were to start from the original images we could add noise or perform affine distortions that approximate changes in the angle of the photo or lighting without creating abnormal facial shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved source for VGG-Face\n",
    "After muching around with VGG-Face trying to get it to work I think I found a better source for a Keras-based VGG-Face model ready for fine tuning here - https://github.com/rcmalli/keras-vggface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
